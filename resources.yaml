---
# Global configuration of computing nodes.
images:
  default: vggp-v60-j322-692e75a7c101-main
  gpu: vggp-v60-gpu-j322-692e75a7c101-main-kernel-4.18.0-477.21.1.el8_8-nvidia
  secure: vggp-v60-secure-j322-692e75a7c101-main
  alma: vggp-v60-j342-4c09f3ebbeac-alma
  htcondor-secondary: vgcn~workers+internal~rockylinux-8.6-x86_64~2023-10-26-43739-htcondor-secondary~ebb20b8~kysrpex_local_build
network: bioinf
secgroups:
  - ufr-ingress
  # interactive-egress: A reduced more stringent egress rule for all nodes
  - interactive-egress
sshkey: cloud3
pubkeys:
  # The public key(s) that will be accepted when SSHing to a machine.
  - "AAAAB3NzaC1yc2EAAAABIwAAAQEAuSG1VOumQhbJfOyalJjS4lPC8eeeL02ld/VI2BFApyMfwbB1QxehY3lMBXt/cBTzqU3MxgJQVzNr/AgjHa5rKn2hSGMfKAdaX2tG686k715bBjRm9rJNhmc8KSH9tVS35M0HwRXMfiGvSmb5qJ6utWRZe6RM2JMIbqqI5Oc4tbzPPVKk1+yvT1JdYuyqAOp2yvQbOqKaXmqOhPdPNaJZMI4o+UHmmb8FH6OTDY27G7X7u07ZPwVi1j+6ZFVMQZqg1RhUdg9kmHpHwMX7P6NcD4G9GsISHIh92eva9xgWYGiS0wUsmOWTNgAzzsfRZjMFep+jB2wup6QN7XpMw97eTw=="

# Behavior of `synchronize.py''
graceful: false

nodes_inventory:
  c1.c28m225d50: 7
  c1.c28m475d50: 19
  c1.c36m100d50: 32
  c1.c36m225d50: 15
  c1.c36m900d50: 1
  c1.c36m975d50: 8
  c1.c60m1975d50: 1
  c1.c120m225d50: 12
  c1.c120m425d50: 22
  c1.c125m425d50: 16
  c1.c28m935d50: 4
  c1.c28m875d50: 2
  g1.c14m40g1d50: 4
  g1.c8m40g1d50: 4

deployment:
  # worker-c120m225-htcondor-secondary:
  #   count: 1 #12
  #   flavor: c1.c120m225d50
  #   group: compute
  #   docker: true
  #   image: htcondor-secondary
  #   secondary_htcondor_cluster: true
  #   volume:
  #     size: 1024
  #     type: default
  #   cgroups:
  #     mem_limit_policy: hard
  #     mem_reserved_size: 2048

  worker-maintenance:
    count: 1
    flavor: m1.large
    group: maintenance
  worker-fastturnaround:
    count: 1
    flavor: m1.xlarge
    group: fast-turnaround
  worker-fetch:
    count: 1
    flavor: c1.c36m100d50
    group: upload
  worker-metadata:
    count: 1
    flavor: m1.large
    group: metadata
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 1024
  worker-interactive:
    count: 8 #8
    flavor: c1.c36m100d50
    group: interactive
    docker: true
    image: secure
    volume:
      size: 1024
      type: default
  #  worker-mothur:
  #    count: 3
  #    flavor: c1.c28m475d50
  #    group: compute_mothur
  #    cgroups:
  #      mem_limit_policy: hard
  #      mem_reserved_size: 2048
  worker-c28m475:
    count: 12 #19
    flavor: c1.c28m475d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c28m225:
    count: 0 #7
    flavor: c1.c28m225d50
    group: compute_test
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c36m100:
    count: 22 #32
    flavor: c1.c36m100d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c36m225:
    count: 15 #15
    flavor: c1.c36m225d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c36m900:
    count: 1 #1 it's a c1.c36m975d50 host with probably a faulty memory bank
    flavor: c1.c36m900d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 2048
  worker-c36m975:
    count: 8 #8
    flavor: c1.c36m975d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 2048
  worker-c28m935:
    count: 4 #4
    flavor: c1.c28m935d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 2048
  worker-c28m875:
    count: 2 #2
    flavor: c1.c28m875d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 2048
  worker-c64m2:
    count: 1 #1
    flavor: c1.c60m1975d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
  worker-c120m225:
    count: 12 #12
    flavor: c1.c120m225d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c120m425-alma:
    count: 2 #22
    flavor: c1.c120m425d50
    group: compute
    docker: true
    image: alma
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c120m425:
    count: 20 #22
    flavor: c1.c120m425d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c125m425:
    count: 16 #16
    flavor: c1.c125m425d50
    group: compute
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: hard
      mem_reserved_size: 2048
  worker-c14m40g1:
    count: 4 #4
    flavor: g1.c14m40g1d50
    group: compute_gpu
    image: gpu
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 1024
  worker-c8m40g1:
    count: 4 #4
    flavor: g1.c8m40g1d50
    group: compute_gpu
    image: gpu
    docker: true
    volume:
      size: 1024
      type: default
    cgroups:
      mem_limit_policy: soft
      mem_reserved_size: 1024

  # Training
  #  training-bio0:
  #    count: 1
  #    flavor: c1.c28m225d50
  #    start: 2022-10-13
  #    end: 2023-12-02
  #    group: training-bio00058m
  training-epfl:
    count: 2
    flavor: c1.c28m225d50
    start: 2023-10-26
    end: 2023-10-27
    group: training-epfl2023
  training-bma2:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-08-28
    end: 2023-12-17
    group: training-bma231-ht23
  training-2023c:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-10-23
    end: 2023-10-24
    group: training-2023ngsmastermicro3
  training-debr:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-09-18
    end: 2023-10-20
    group: training-debrecen2023
  training-tbge:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-10-04
    end: 2023-12-30
    group: training-tbgenom2324
  training-gqa2:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-11-13
    end: 2023-12-01
    group: training-gqa23
  training-fhnw:
    count: 2
    flavor: c1.c28m225d50
    start: 2023-12-06
    end: 2023-12-13
    group: training-fhnw-genomics-2023
  training-ga-e:
    count: 3
    flavor: c1.c28m225d50
    start: 2024-01-28
    end: 2024-02-03
    group: training-ga-embo24
  training-inte:
    count: 2
    flavor: c1.c28m225d50
    start: 2023-10-19
    end: 2023-10-20
    group: training-internat23
  training-qub-:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-10-19
    end: 2023-10-23
    group: training-qub-glxy-23
  training-gm2-:
    count: 1
    flavor: c1.c28m225d50
    start: 2023-11-01
    end: 2023-11-03
    group: training-gm2-practicals
  training-argh:
    count: 2
    flavor: c1.c28m225d50
    start: 2023-11-13
    end: 2023-11-17
    group: training-arghackathon
  training-rna-:
    count: 2
    flavor: c1.c28m225d50
    start: 2023-11-27
    end: 2023-12-22
    group: training-rna-seq-vu
